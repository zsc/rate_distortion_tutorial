# 第三章：经典信源的率失真函数

本章详细计算两个最重要的信源的率失真函数：二元对称源（伯努利源）在汉明失真下，以及连续高斯源在均方误差失真下。这两个经典结果是率失真理论的基石，它们的推导和性质为理解更复杂信源提供了重要直觉。

**学习目标**：
- 掌握伯努利源的率失真函数推导和性质
- 理解高斯源率失真函数的"水注入"解释
- 建立对离散和连续信源率失真权衡的直觉
- 了解这些经典结果在实际中的应用

---

## 3.1 伯努利源的率失真函数

### 3.1.1 问题设定

考虑**伯努利($p$)源**：$X \in \{0, 1\}$，$P(X=1) = p$，$P(X=0) = 1-p$。

使用**汉明失真**：

$$d_H(x, \hat{x}) = \begin{cases} 0 & \text{if } x = \hat{x} \\ 1 & \text{if } x \neq \hat{x} \end{cases}$$

目标：计算率失真函数 $R(D)$。

**物理意义**：汉明失真对应"错误率"。失真 $D$ 就是平均错误概率，$R(D)$ 给出了在错误率不超过 $D$ 的情况下压缩所需的最小码率。

### 3.1.2 对称情况：$p = 0.5$

我们先考虑最简单的情况：$p = 0.5$（均匀分布）。此时 $H(X) = 1$ 比特。

**定理 3.1**（伯努利(0.5)源的率失真函数）：

$$R(D) = \begin{cases}
1 - H_b(D) & 0 \leq D \leq 0.5 \\
0 & D > 0.5
\end{cases}$$

其中 $H_b(D) = -D \log D - (1-D) \log(1-D)$ 是二元熵函数。

**解释**：
- 当 $D = 0$（无损）：$R(0) = 1 - H_b(0) = 1$，等于信源熵
- 当 $D = 0.5$（随机猜测）：$R(0.5) = 1 - H_b(0.5) = 1 - 1 = 0$，不需要传输信息
- 中间：率和失真平滑权衡

**推导思路**（不含完整证明）：

最优测试信道 $p(\hat{x}|x)$ 具有对称结构：

$$p(\hat{x}=x|x) = 1 - \delta, \quad p(\hat{x} \neq x|x) = \delta$$

即以概率 $\delta$ 翻转。此时：
- 失真：$D = \delta$
- $\hat{X}$ 的分布仍是均匀的（对称性）
- 互信息：$I(X; \hat{X}) = H(\hat{X}) - H(\hat{X}|X) = 1 - H_b(\delta) = 1 - H_b(D)$

可以证明这个测试信道是最优的，因此 $R(D) = 1 - H_b(D)$。

**率失真曲线**：

```
R(D)
 1.0 |*
     | \
     |  \
 0.8 |   \
     |    \
 0.6 |     \
     |      \
 0.4 |       \
     |        \
 0.2 |         \
     |          \____
 0.0 |_______________*___
     0   0.1  0.2  0.3  0.4  0.5   D
```

### 3.1.3 一般情况：任意 $p$

对于一般的伯努利($p$)源，率失真函数更复杂：

$$R(D) = \begin{cases}
H_b(p) - H_b(D) & 0 \leq D \leq \min(p, 1-p) \\
0 & D > \min(p, 1-p)
\end{cases}$$

**关键观察**：
- 无失真：$R(0) = H_b(p)$，等于信源熵
- 最大失真：$D_{\max} = \min(p, 1-p)$，对应总是猜测概率较大的符号
- 例如，如果 $p = 0.1$，最优常数重建是总猜 0，错误率 $D_{\max} = 0.1$

**Rule of thumb**：对于汉明失真，$R(D) = H(X) - H(D)$ 形式很常见。直观理解：$H(X)$ 是总的不确定性，$H(D)$ 对应失真引入的"额外"不确定性，剩下的是必须传输的信息。

### 3.1.4 最优测试信道的结构

最优测试信道 $p^*(\hat{x}|x)$ 满足：

$$p^*(\hat{x}=1|x=1) = p^*(\hat{x}=0|x=0) = 1 - \delta$$

其中 $\delta$ 由失真约束决定。这个对称结构反映了汉明失真的对称性。

**重要性质**：最优重建 $\hat{X}$ 仍是伯努利分布，但参数不同。通过适当的"加噪"（以概率 $\delta$ 翻转），在满足失真约束的同时最小化互信息。

---

## 3.2 高斯源的率失真函数

### 3.2.1 问题设定

考虑**高斯源**：$X \sim \mathcal{N}(0, \sigma^2)$（均值为0，方差为 $\sigma^2$ 的高斯随机变量）。

使用**平方误差失真**：

$$d(x, \hat{x}) = (x - \hat{x})^2$$

目标：计算率失真函数 $R(D)$。

这是连续信源的经典例子，在图像、音频、通信等领域有广泛应用。

### 3.2.2 主要结果

**定理 3.2**（高斯源的率失真函数）：

对于 $X \sim \mathcal{N}(0, \sigma^2)$ 和平方误差失真，率失真函数为：

$$R(D) = \begin{cases}
\frac{1}{2} \log \frac{\sigma^2}{D} & 0 < D \leq \sigma^2 \\
0 & D > \sigma^2
\end{cases}$$

其中对数以2为底（如果用自然对数，系数变为 $\frac{1}{2\ln 2}$）。

**关键特征**：
- $R(D)$ 随 $D$ 对数递减
- 当 $D = \sigma^2$ 时，$R(D) = 0$（对应零重建）
- 当 $D \to 0$ 时，$R(D) \to \infty$（完全无损需要无穷码率）

**单位注意**：对于连续源，$R(D)$ 可能是负的（如果用微分熵），但上式中 $R(D) \geq 0$ 因为 $D \leq \sigma^2$。

### 3.2.3 最优测试信道：高斯加噪

高斯源的一个优美性质是：**最优测试信道也是高斯的**。

具体地，最优重建为：

$$\hat{X} = X + Z$$

其中 $Z \sim \mathcal{N}(0, \sigma_Z^2)$ 独立于 $X$。即在原始信号上加高斯噪声！

**失真**：

$$D = \mathbb{E}[(X - \hat{X})^2] = \mathbb{E}[Z^2] = \sigma_Z^2$$

**互信息**：

对于高斯随机变量，有公式：

$$I(X; \hat{X}) = \frac{1}{2} \log \frac{\text{Var}(\hat{X})}{\text{Var}(\hat{X}|X)}$$

由于 $\hat{X} = X + Z$：
- $\text{Var}(\hat{X}) = \sigma^2 + \sigma_Z^2 = \sigma^2 + D$
- $\text{Var}(\hat{X}|X) = \sigma_Z^2 = D$

因此：

$$I(X; \hat{X}) = \frac{1}{2} \log \frac{\sigma^2 + D}{D}$$

但这不对！正确的计算应该是：

$$I(X; X+Z) = h(X) - h(X|X+Z) = h(X) - h(Z)$$

其中 $h(\cdot)$ 是微分熵。对于高斯 $\mathcal{N}(0, \sigma^2)$，$h(X) = \frac{1}{2}\log(2\pi e \sigma^2)$。

简化后（利用高斯性质），得到：

$$I(X; \hat{X}) = \frac{1}{2} \log \left(1 + \frac{\sigma^2}{D}\right) = \frac{1}{2} \log \frac{\sigma^2 + D}{D}$$

**但我们要的是反向**：固定 $D$，最小化 $I(X; \hat{X})$。对于高斯源，上式已经是最小值（高斯测试信道最优），因此：

等等，让我重新整理。实际上，最优方案是：

$$\hat{X} = aX$$

其中 $a$ 是缩放因子。但这不对...

**正确的表述**（逆反水注入）：

最优重建满足：

$$\hat{X} = X - Z$$

其中 $Z \sim \mathcal{N}(0, D)$ 独立于 $X$，且 $Z$ 称为**量化噪声**。

此时失真为 $D$，互信息为：

$$I(X; \hat{X}) = h(\hat{X}) - h(\hat{X}|X)$$

由于 $\hat{X}|X$ 等价于 $-Z$，$h(\hat{X}|X) = h(Z) = \frac{1}{2}\log(2\pi e D)$。

而 $\hat{X} = X - Z$，如果 $X$ 和 $Z$ 独立且都是高斯，则 $\hat{X}$ 也是高斯，$\hat{X} \sim \mathcal{N}(0, \sigma^2 + D)$（不对，应该是 $\sigma^2 - D$ 如果设计得当...）

**标准推导**（基于拉格朗日）：

通过变分法，可以证明最优 $p(\hat{x}|x)$ 使得 $\hat{X} \sim \mathcal{N}(0, \sigma^2 - D)$（当 $D < \sigma^2$ 时），且：

$$\hat{X} = \frac{\sigma^2 - D}{\sigma^2} X + \text{noise}$$

最终互信息为：

$$I(X; \hat{X}) = \frac{1}{2} \log \frac{\sigma^2}{D}$$

这就是 $R(D)$。

**Rule of thumb**：高斯源的率失真函数是对数形式 $R(D) = c \log \frac{\sigma^2}{D}$。每增加1比特（码率增加1），失真减少一半（$D \to D/2$）。这个"6 dB/bit"规律在实际压缩中很常见。

### 3.2.4 水注入解释（逆反定理）

高斯源的结果可以推广到**多维高斯源**（高斯向量），此时有著名的**水注入（water-filling）**解法。

考虑 $X \sim \mathcal{N}(0, \mathbf{K}_X)$，其中 $\mathbf{K}_X$ 是协方差矩阵。设其特征值为 $\lambda_1, ..., \lambda_k$（按降序）。

**逆反水注入定理**：率失真函数为：

$$R(D) = \frac{1}{2} \sum_{i=1}^k \max\left\\{0, \log \frac{\lambda_i}{\theta}\right\\}$$

其中 $\theta$ 由失真约束决定：

$$D = \sum_{i=1}^k \min(\lambda_i, \theta)$$

**水注入类比**：
- 想象有 $k$ 个容器，第 $i$ 个容器底部高度是 $0$，宽度相同
- 向容器中注水，总水量对应失真 $D$
- 水面高度是 $\theta$
- 第 $i$ 个容器的水量是 $\min(\lambda_i, \theta)$
- 码率对应"空气部分"：$\log(\lambda_i/\theta)$（当 $\lambda_i > \theta$）

```
逆反水注入示意（简化）：
    |          |
    |   空气   |  <- 码率分配给此分量
────┼──────────┼──  θ (水面/失真阈值)
    |   水     |
    |   水     |  <- 失真分配给此分量
    └──────────┘
    分量i (λ_i大)
```

**直觉**：
- 方差大的分量（$\lambda_i$ 大）：失真少，码率多（重要分量，精细编码）
- 方差小的分量（$\lambda_i$ 小）：失真多，码率少（不重要分量，粗糙编码或丢弃）

这在图像压缩（DCT、KLT）中直接应用：高方差系数精细量化，低方差系数粗糙量化或丢弃。

---

## 3.3 两种信源的对比

| 特性 | 伯努利源（汉明失真） | 高斯源（平方误差） |
|:---|:---:|:---:|
| **信源类型** | 离散 | 连续 |
| **$R(D)$ 形式** | $H_b(p) - H_b(D)$ | $\frac{1}{2}\log\frac{\sigma^2}{D}$ |
| **最优测试信道** | 对称二元信道（翻转） | 高斯加噪 |
| **$R(D)$ 形状** | 凸，有限支撑 | 凸，对数递减 |
| **$D_{\max}$** | $\min(p, 1-p)$ | $\sigma^2$ |
| **实际应用** | 文本、离散数据 | 图像、音频、传感器数据 |

**共同特点**：
1. 都是凸函数
2. 都从 $R(0) = H(X)$ 单调递减到 $R(D_{\max}) = 0$
3. 最优测试信道都有简洁的解析形式

---

## 3.4 实际应用与推广

### 3.4.1 图像压缩中的高斯假设

虽然图像像素不是完全高斯的，但**DCT 系数**或**小波系数**在高频部分近似高斯分布。因此，高斯源的率失真函数为图像压缩提供了理论指导。

**JPEG 的量化**：
- 将图像分块，对每块做 DCT
- 高频系数近似高斯，使用 $R(D) = \frac{1}{2}\log\frac{\sigma^2}{D}$ 指导量化步长
- 低频系数（方差大）分配更多比特，高频系数（方差小）分配更少比特或丢弃

### 3.4.2 拉普拉斯源和其他分布

除了高斯和伯努利，其他常见分布的率失真函数：

**拉普拉斯源**：$p(x) = \frac{1}{2b} e^{-|x|/b}$，平方误差失真：

$$R(D) = \log \frac{b^2}{D} - 1 \quad (D \leq D_0)$$

其中 $D_0$ 是阈值。拉普拉斯分布在图像/视频的预测误差中常见。

**均匀源**：$X \sim \text{Uniform}[-a, a]$，平方误差：

$$R(D) = \log \frac{a}{\sqrt{3D}} \quad (D \leq a^2/3)$$

### 3.4.3 从单信源到向量源

单变量的结果可以推广到向量（多维）情况。关键工具是**KLT（Karhunen-Loève Transform）**或**PCA（主成分分析）**：

1. 对信源向量做 KLT，得到不相关的分量
2. 每个分量独立应用标量率失真函数
3. 水注入分配码率

这是**变换编码**的理论基础，在 JPEG、MPEG 等标准中广泛应用。

**Rule of thumb**：对于大多数自然信号（图像、音频），高斯假设 + 变换编码 + 水注入分配是一个强大的压缩框架。即使信号不完全高斯，这个框架也能给出接近最优的性能。

---

## 3.5 本章小结

**核心结果**：

1. **伯努利(0.5)源，汉明失真**：
   $$R(D) = 1 - H_b(D), \quad 0 \leq D \leq 0.5$$
   - 最优测试信道：对称翻转信道
   - $R(D)$ 形式：$H(X) - H(D)$

2. **高斯源，平方误差失真**：
   $$R(D) = \frac{1}{2} \log \frac{\sigma^2}{D}, \quad 0 < D \leq \sigma^2$$
   - 最优测试信道：高斯加噪
   - 多维推广：逆反水注入定理

**关键公式**：

- 二元熵函数：$H_b(p) = -p\log p - (1-p)\log(1-p)$
- 伯努利源：$R(D) = H_b(p) - H_b(D)$
- 高斯源：$R(D) = \frac{1}{2}\log\frac{\sigma^2}{D}$
- 水注入：$R(D) = \frac{1}{2}\sum_i \max\\{0, \log\frac{\lambda_i}{\theta}\\}$

**实际意义**：

- 伯努利源模型适用于离散数据、二值数据、通信中的错误率分析
- 高斯源模型适用于图像、音频、传感器数据等连续信号
- 水注入原理指导实际编码器的比特分配

---

## 3.6 常见陷阱与错误

### Gotcha #1: 高斯源的 $R(D)$ 可以是负的？

**错误**：直接使用微分熵 $h(X)$ 时，$R(D)$ 可能是负值。

**正解**：微分熵本身可以是负的（不同于离散熵）。率失真函数 $R(D)$ 也可以是负值，这不矛盾。$R(D)$ 表示的是**相对**于参考的码率，而不是绝对码率。实际编码时，总码率 = $R(D)$ + 熵编码开销 + 其他开销，总和是非负的。

实际上，对于 $D < \sigma^2$，$R(D) = \frac{1}{2}\log\frac{\sigma^2}{D} > 0$ 总是正的。只有当我们考虑非常特殊的失真度量或归一化时，$R(D)$ 才可能为负。

### Gotcha #2: 对数的底数

**错误**：混淆以2为底和自然对数。

**正解**：信息论中通常使用以2为底的对数（码率单位：比特）。如果使用自然对数（码率单位：奈特 nat），公式会有常数因子差异。例如：

- 以2为底：$R(D) = \frac{1}{2} \log_2 \frac{\sigma^2}{D}$ 比特
- 自然对数：$R(D) = \frac{1}{2} \ln \frac{\sigma^2}{D}$ 奈特 = $\frac{1}{2\ln 2} \log_2 \frac{\sigma^2}{D}$ 比特

务必统一底数，避免混淆。

### Gotcha #3: 平方误差失真的单位

**错误**：忘记失真 $D$ 的单位。

**正解**：对于平方误差失真，$D$ 的单位是"（原信号单位）$^2$"。例如，如果 $X$ 的单位是伏特，则 $D$ 的单位是伏特$^2$（功率）。在比较不同系统时，必须注意信号幅度的归一化。

通常，我们归一化信号使 $\sigma^2 = 1$，此时失真 $D$ 可以理解为相对失真（失真功率/信号功率），这就是**信噪比（SNR）**的倒数。

### Gotcha #4: 高斯假设的适用性

**错误**：认为所有信号都是高斯的，因此直接应用 $R(D) = \frac{1}{2}\log\frac{\sigma^2}{D}$。

**正解**：高斯假设是近似。对于非高斯源（如拉普拉斯分布），率失真函数会有所不同。但实际中，由于中心极限定理和变换后的近似高斯性，高斯模型通常是合理的一阶近似。

对于明显非高斯的源（如稀疏信号、离散符号），需要更精确的模型（第四章介绍数值计算方法）。

### Gotcha #5: 最优测试信道的实现

**错误**：认为实际编码器必须显式实现测试信道 $p(\hat{x}|x)$（例如，给信号加噪声）。

**正解**：测试信道是理论工具，描述最优编码的**统计特性**，而非具体实现。实际量化器（如标量量化、矢量量化）在长序列上近似这个统计行为，但不需要显式加噪声。

例如，标量量化器 $\hat{x} = Q(x)$ 是确定性的，但当 $x$ 在量化区间内均匀分布时，量化误差 $x - Q(x)$ 近似均匀噪声，从而近似最优测试信道。

### Gotcha #6: 伯努利源的对称性

**错误**：对于伯努利($p$)源，$p \neq 0.5$ 时，认为最优测试信道仍是对称的。

**正解**：当 $p \neq 0.5$ 时，最优测试信道**不再是完全对称的**。具体地，$p(1|0)$ 和 $p(0|1)$ 会不同，以反映信源的不对称性。完整分析需要拉格朗日方法（或 Blahut-Arimoto 算法）。

不过，率失真函数的形式 $R(D) = H_b(p) - H_b(D)$ 仍成立（对 $D \leq \min(p,1-p)$），这是一个美妙的结果。

### Gotcha #7: 水注入的方向

**错误**：混淆"水注入"（信道容量）和"逆反水注入"（率失真）。

**正解**：
- **信道容量**（功率分配）：水注入（water-filling），向好信道注水（分配更多功率/码率）
- **率失真**（失真分配）：逆反水注入（reverse water-filling），向小方差分量注水（分配更多失真）

两者是对偶的，但方向相反。在信道容量中我们**最大化**互信息，在率失真中我们**最小化**互信息。

---

**下一章预告**：第四章将介绍 Blahut-Arimoto 算法，用于数值计算没有闭式解的率失真函数，并扩展到矢量量化和多维信源。

[← 第二章](chapter2.md) | [返回目录](index.md) | [第四章：率失真计算与矢量量化 →](chapter4.md)
