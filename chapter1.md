# 第一章：信息论基础与率失真入门

本章回顾信息论的核心概念，包括熵、联合熵、条件熵和互信息，并在此基础上引入率失真理论。我们将了解有损压缩的基本权衡，学习如何选择失真度量，以及率失真函数的定义。通过简单例子建立对率失真理论的直觉理解。

**学习目标**：
- 掌握熵和互信息的物理意义
- 理解有损压缩的必要性和基本思想
- 学会选择合适的失真度量
- 理解率失真函数的定义和意义

---

## 1.1 信息论基础回顾

### 1.1.1 熵：不确定性的度量

**熵**（Entropy）是信息论的核心概念，它度量了一个随机变量的不确定性。对于离散随机变量 $X$，其熵定义为：

$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$$

其中 $\mathcal{X}$ 是 $X$ 的取值空间，$p(x)$ 是概率质量函数。按照惯例，$0 \log 0 = 0$。

**物理意义**：熵 $H(X)$ 表示用最优编码方案对 $X$ 进行无损压缩时，平均每个符号所需的最少比特数。换句话说，熵是信源的"固有压缩极限"。

**示例**：考虑一个二元随机变量 $X \in \{0, 1\}$，$P(X=1) = p$。其熵为：

$$H(X) = -p \log p - (1-p) \log(1-p) \triangleq H_b(p)$$

这个函数称为**二元熵函数**（binary entropy function）。当 $p = 0.5$ 时，$H_b(0.5) = 1$ 比特，不确定性最大；当 $p = 0$ 或 $p = 1$ 时，$H_b(p) = 0$，没有不确定性。

**Rule of thumb**：熵在均匀分布时达到最大值 $\log |\mathcal{X}|$，在确定性分布（某个值概率为1）时达到最小值 0。

### 1.1.2 联合熵与条件熵

对于两个随机变量 $X$ 和 $Y$，**联合熵**定义为：

$$H(X, Y) = -\sum_{x,y} p(x,y) \log p(x,y)$$

**条件熵** $H(Y|X)$ 表示在已知 $X$ 的情况下 $Y$ 的平均不确定性：

$$H(Y|X) = \sum_{x} p(x) H(Y|X=x) = -\sum_{x,y} p(x,y) \log p(y|x)$$

**链式法则**：

$$H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$

这个公式的直观解释是：两个变量的联合不确定性 = 第一个变量的不确定性 + 已知第一个变量后第二个变量的剩余不确定性。

### 1.1.3 互信息：共享的信息

**互信息**（Mutual Information）度量了两个随机变量之间的相关性：

$$I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

互信息有多种等价形式：

$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)$$

**物理意义**：$I(X; Y)$ 表示通过观测 $Y$ 能够减少的关于 $X$ 的不确定性，或者说 $X$ 和 $Y$ "共享"的信息量。

**对称性**：$I(X; Y) = I(Y; X)$，这是互信息的重要性质。

**非负性**：$I(X; Y) \geq 0$，当且仅当 $X$ 和 $Y$ 独立时等号成立。

```
文氏图示意（ASCII）：
         H(X)           H(Y)
      ┌────────┐     ┌────────┐
      │        │     │        │
      │   H(X|Y)  I(X;Y)  H(Y|X) │
      │        │     │        │
      └────────┘     └────────┘
         └──────┬──────┘
            H(X,Y)
```

### 1.1.4 数据处理不等式

**数据处理不等式**（Data Processing Inequality）是信息论的基本定理之一：

如果随机变量形成马尔可夫链 $X \to Y \to Z$（即给定 $Y$，$X$ 和 $Z$ 条件独立），则：

$$I(X; Z) \leq I(X; Y)$$

**直观理解**：对数据的任何处理（$Y \to Z$）都不能增加信息。这个不等式在率失真理论中扮演重要角色。

**Rule of thumb**：信息处理只会丢失信息，不能创造信息。编码、压缩、传输等过程最多保持信息不变，通常会减少信息。

---

## 1.2 有损压缩的基本问题

### 1.2.1 无损压缩的局限

根据香农的无损编码定理，对信源 $X$ 进行无损压缩时，平均码长不能小于熵 $H(X)$。这对许多实际应用来说压缩率不够。

例如，高分辨率图像、视频、音频等数据即使经过最优无损压缩，数据量仍然很大。实际应用中（如流媒体、存储），我们常常可以容忍一定的失真换取更高的压缩率。

### 1.2.2 率失真问题的提出

**核心问题**：在允许一定失真 $D$ 的情况下，对信源进行有损压缩所需的最小比特率是多少？

这就是**率失真理论**要回答的基本问题。率失真理论研究的是**率**（Rate，压缩后的比特率）和**失真**（Distortion，压缩带来的质量损失）之间的根本权衡。

**直观理解**：
- 如果我们要求完全无损（$D = 0$），则需要的比特率至少是 $H(X)$
- 如果我们允许完全失真（比如把所有信号都压缩成常数），则比特率可以是 0
- 在这两个极端之间，必然存在一条描述率失真权衡的曲线

### 1.2.3 编码器-解码器框架

率失真编码的基本框架：

```
    信源        编码器              解码器        重建
    X^n  ───→  f_n: X^n → {1,...,M}  ───→  g_n: {1,...,M} → X̂^n  ───→  X̂^n
                   (压缩)                        (解压)

    码率 R = (log M) / n bits/symbol
    失真 D = (1/n) Σ d(X_i, X̂_i)
```

- **信源**：产生长度为 $n$ 的序列 $X^n = (X_1, ..., X_n)$
- **编码器** $f_n$：将 $X^n$ 映射到 $\{1, 2, ..., M\}$ 中的某个索引（码字）
- **解码器** $g_n$：将索引映射回重建序列 $\hat{X}^n$
- **码率**：$R = \frac{\log M}{n}$ 比特/符号（$M$ 个码字需要 $\log M$ 比特）
- **失真**：$D = \frac{1}{n} \sum_{i=1}^n d(X_i, \hat{X}_i)$，其中 $d(\cdot, \cdot)$ 是失真度量

---

## 1.3 失真度量的选择

失真度量 $d(x, \hat{x})$ 量化了原始符号 $x$ 和重建符号 $\hat{x}$ 之间的"差异"。不同应用需要不同的失真度量。

### 1.3.1 常见失真度量

**1. 汉明失真（Hamming Distortion）**

用于离散信源，定义为：

$$d_H(x, \hat{x}) = \begin{cases} 0 & \text{if } x = \hat{x} \\ 1 & \text{if } x \neq \hat{x} \end{cases}$$

**适用场景**：文本压缩、离散数据，关心的是"是否相等"而非"差多少"。

**2. 平方误差失真（Squared Error Distortion）**

用于连续或实值信源：

$$d_{SE}(x, \hat{x}) = (x - \hat{x})^2$$

对于向量信源 $\mathbf{x} \in \mathbb{R}^k$：

$$d_{SE}(\mathbf{x}, \hat{\mathbf{x}}) = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 = \sum_{i=1}^k (x_i - \hat{x}_i)^2$$

这是最常用的失真度量，也称为**均方误差**（MSE, Mean Squared Error）。

**适用场景**：图像、音频、传感器数据等连续值信号。

**3. 绝对误差失真（Absolute Error Distortion）**

$$d_{AE}(x, \hat{x}) = |x - \hat{x}|$$

相比平方误差，对大误差的惩罚较轻。

**4. 加权失真**

某些维度或区域可能更重要，可以使用加权：

$$d_W(\mathbf{x}, \hat{\mathbf{x}}) = \sum_{i=1}^k w_i (x_i - \hat{x}_i)^2$$

### 1.3.2 失真度量的性质

一个好的失真度量通常应该满足：

1. **非负性**：$d(x, \hat{x}) \geq 0$
2. **同一性**：$d(x, x) = 0$

注意失真度量**不要求**对称性（$d(x, \hat{x}) \neq d(\hat{x}, x)$）或三角不等式。

### 1.3.3 平均失真

对于长度为 $n$ 的序列，平均失真定义为：

$$D = \frac{1}{n} \sum_{i=1}^n d(x_i, \hat{x}_i)$$

由于 $X^n$ 和 $\hat{X}^n$ 是随机的，我们关心的是**期望失真**：

$$\mathbb{E}[D] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[d(X_i, \hat{X}_i)]$$

对于独立同分布（i.i.d.）信源，这简化为：

$$\mathbb{E}[D] = \mathbb{E}[d(X, \hat{X})]$$

**Rule of thumb**：失真度量的选择应该反映应用的实际需求。MSE 适合信号处理，但对于图像/视频，感知失真度量（如 SSIM）往往更合适（将在第五章详述）。

---

## 1.4 率失真函数的定义

### 1.4.1 率失真函数 $R(D)$

对于给定的失真水平 $D$，**率失真函数** $R(D)$ 定义为：

$$R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d(X,\hat{X})] \leq D} I(X; \hat{X})$$

其中最小化是在所有满足平均失真约束的条件分布 $p(\hat{x}|x)$ 上进行的。

**直观解释**：
- $p(\hat{x}|x)$ 称为**测试信道**（test channel），描述了给定原始符号 $x$ 时，如何随机选择重建符号 $\hat{x}$
- $I(X; \hat{X})$ 度量了 $X$ 和 $\hat{X}$ 之间的"相关程度"，可以理解为需要传输的信息量
- $R(D)$ 给出了在平均失真不超过 $D$ 的约束下，最少需要传输多少信息

### 1.4.2 率失真定理（非正式陈述）

率失真定理告诉我们：

**对于任意 $\epsilon > 0$ 和足够大的 $n$**，存在编码-解码方案 $(f_n, g_n)$ 使得：
- 码率 $R \leq R(D) + \epsilon$
- 平均失真 $\mathbb{E}[D] \leq D$

并且不存在码率小于 $R(D)$ 的编码方案能达到失真 $D$。

**意义**：$R(D)$ 刻画了率失真权衡的理论极限。这是信息论给出的"基本界限"。

### 1.4.3 简单例子：二元对称源

考虑伯努利(0.5)源：$X \in \{0, 1\}$，$P(X=0) = P(X=1) = 0.5$，汉明失真。

可以证明（详见第三章），其率失真函数为：

$$R(D) = \begin{cases}
1 - H_b(D) & 0 \leq D \leq 0.5 \\
0 & D > 0.5
\end{cases}$$

其中 $H_b(D) = -D \log D - (1-D) \log(1-D)$ 是二元熵函数。

**解读**：
- 当 $D = 0$（无损）时，$R(0) = 1$ 比特，等于信源熵 $H(X) = 1$
- 当 $D = 0.5$ 时，$R(0.5) = 0$，此时平均有一半的符号是错的，相当于随机猜测，不需要传输任何信息
- 在中间，率和失真平滑地权衡

```
率失真曲线示意（ASCII）：
R(D)
 1 |*
   | *
   |  *
   |   *
0.5|    *
   |     *
   |      *
   |       *******
 0 |____________*______
   0    0.25    0.5    D
```

**Rule of thumb**：率失真函数 $R(D)$ 是单调递减的凸函数。失真允许越大，所需码率越小。

---

## 1.5 率失真与互信息的联系

率失真函数的定义涉及互信息 $I(X; \hat{X})$。这不是巧合，而是深刻的联系：

**互信息的意义**：$I(X; \hat{X})$ 度量了从 $\hat{X}$ 能"推断"出多少关于 $X$ 的信息。在率失真编码中：
- 编码器需要传输足够的信息让解码器能重建 $\hat{X}$
- 为了使 $\hat{X}$ 接近 $X$（低失真），$\hat{X}$ 必须包含关于 $X$ 的足够信息
- 但信息越多，需要的码率越高

率失真函数的定义正是在这个权衡中找最优点：对于给定失真 $D$，找最小的互信息（即最小的码率）。

---

## 1.6 本章小结

**核心概念**：

1. **熵** $H(X)$：信源的不确定性度量，无损压缩的理论下界
2. **互信息** $I(X; Y)$：两个随机变量共享的信息量，具有对称性和非负性
3. **数据处理不等式**：信息处理不能增加信息，只能减少或保持
4. **率失真问题**：在允许失真 $D$ 的情况下，有损压缩所需的最小码率是多少？
5. **失真度量**：量化原始信号和重建信号的差异，常见的有汉明失真和平方误差失真
6. **率失真函数**：$R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d] \leq D} I(X; \hat{X})$，刻画率失真权衡的理论极限

**关键公式**：

- 熵：$H(X) = -\sum_x p(x) \log p(x)$
- 条件熵：$H(Y|X) = -\sum_{x,y} p(x,y) \log p(y|x)$
- 互信息：$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$
- 率失真函数：$R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d(X,\hat{X})] \leq D} I(X; \hat{X})$

---

## 1.7 常见陷阱与错误

### Gotcha #1: 混淆熵和率失真函数

**错误**：认为 $R(D)$ 在 $D=0$ 时等于 $\log |\mathcal{X}|$。

**正解**：$R(0) = H(X)$，不是 $\log |\mathcal{X}|$。只有当 $X$ 均匀分布时两者才相等。熵 $H(X)$ 考虑了信源的概率分布，而 $\log |\mathcal{X}|$ 只是字母表大小。

### Gotcha #2: 失真度量必须是距离

**错误**：认为失真度量必须满足对称性和三角不等式（即必须是距离度量）。

**正解**：失真度量只需满足非负性和 $d(x,x) = 0$。对称性和三角不等式不是必需的。例如，在某些应用中，过估（$\hat{x} > x$）和低估（$\hat{x} < x$）的代价可能不同，此时失真度量可以是非对称的。

### Gotcha #3: 期望失真 vs 最大失真

**错误**：将期望失真约束 $\mathbb{E}[d(X,\hat{X})] \leq D$ 与最坏情况约束 $\max d(X,\hat{X}) \leq D$ 混淆。

**正解**：率失真理论通常使用期望失真（平均意义下的失真）。最坏情况失真会导致不同的理论（worst-case distortion）。期望失真允许少数样本失真较大，只要平均失真不超过 $D$。

### Gotcha #4: 独立性假设

**错误**：认为率失真理论只适用于 i.i.d. 信源。

**正解**：虽然经典结果（如第三章的伯努利源和高斯源）通常假设 i.i.d.，但率失真理论可以推广到具有记忆的信源（如马尔可夫源）。推广的关键是考虑序列的联合分布而非单符号分布。

### Gotcha #5: 率失真函数不可计算？

**错误**：因为率失真函数的定义涉及无穷维优化（优化所有可能的条件分布 $p(\hat{x}|x)$），所以认为无法计算。

**正解**：对于许多重要的信源（如高斯源、伯努利源），率失真函数有闭式解。对于一般信源，Blahut-Arimoto 算法（第四章）提供了有效的数值计算方法。

### Gotcha #6: 码率单位

**错误**：混淆"比特/符号"和"总比特数"。

**正解**：率失真函数 $R(D)$ 的单位是**比特/符号**（bits per symbol）。对于长度为 $n$ 的序列，总比特数约为 $nR(D)$。务必区分"每符号的平均比特数"和"总比特数"。

---

**下一章预告**：第二章将深入探讨率失真函数的数学性质（凸性、单调性），证明率失真定理，并揭示率失真理论与信道编码的对偶关系。

[← 返回目录](index.md) | [第二章：率失真定理与理论性质 →](chapter2.md)
